# Projeto TupiAntigo2LLM: Fine-tuning de LLM com DicionÃ¡rio Tupi Antigo

**(Status Atual: Fine-tuning inicial concluÃ­do, avaliaÃ§Ã£o em andamento)**

![](https://miro.medium.com/v2/resize:fit:720/format:webp/1*wXtFuUNYL6Zr07efqa-X_A.png)

## 1. Objetivo

Este projeto visa criar um dataset estruturado a partir de um dicionÃ¡rio Tupi Antigo-PortuguÃªs digitalizado (via OCR) e utilizÃ¡-lo para realizar o fine-tuning de um Modelo de Linguagem Grande (LLM). O objetivo Ã© desenvolver um modelo capaz de compreender e realizar tarefas bÃ¡sicas relacionadas ao Tupi Antigo, como fornecer definiÃ§Ãµes de vocabulÃ¡rio e, potencialmente, auxiliar na preservaÃ§Ã£o digital e estudo dessa lÃ­ngua ancestral brasileira.

## 2. Fonte dos Dados

O material base principal Ã© uma versÃ£o PDF OCRizada do:

*   **DicionÃ¡rio Tupi Antigo-PortuguÃªs** de Carvalho, Moacyr Ribeiro de. (1987). Salvador: BDA.

Localizado em: `Files/txt/Carvalho_1987_DicTupiAntigo-Port_OCR.pdf`

## 3. Estrutura de Pastas (Atualizada)
```
TUPI2LLM/
â”œâ”€â”€ .venv/ # Ambiente virtual Python
â”œâ”€â”€ Files/
â”‚ â””â”€â”€ txt/ # Arquivos de texto fonte (PDF, TXT bruto)
â”‚ â”œâ”€â”€ Carvalho_1987_DicTupiAntigo-Port_OCR.pdf
â”‚ â””â”€â”€ Carvalho_1987_DicTupiAntigo-Port_OCR.txt
â”œâ”€â”€ output/ # Arquivos gerados pelo processamento
â”‚ â”œâ”€â”€ alfabeto/ # Processamento da seÃ§Ã£o do alfabeto/gramÃ¡tica
â”‚ â”‚ â”œâ”€â”€ alfabeto_bruto.txt
â”‚ â”‚ â””â”€â”€ alfabeto_tratato.txt
â”‚ â”œâ”€â”€ Arquivos_treinamento/ # Datasets formatados para LLM e logs
â”‚ â”‚ â”œâ”€â”€ alfabeto_tratado.jsonl # InstruÃ§Ãµes de GramÃ¡tica
â”‚ â”‚ â”œâ”€â”€ dicionario_estruturado_final_vX.json # JSON estruturado (usar versÃ£o final, ex: v8)
â”‚ â”‚ â”œâ”€â”€ dataset_instrucao_vX_final.jsonl # Dataset principal de instruÃ§Ãµes (definiÃ§Ãµes)
â”‚ â”‚ â”œâ”€â”€ train_dataset.jsonl # Dataset de Treino (divisÃ£o do JSONL de instruÃ§Ã£o)
â”‚ â”‚ â””â”€â”€ validation_dataset.jsonl # Dataset de ValidaÃ§Ã£o (divisÃ£o do JSONL de instruÃ§Ã£o)
â”‚ â”‚ â””â”€â”€ dataset_tupi_vX_final.txt # (Opcional) Dataset de texto Tupi puro
â”‚ â”‚ â””â”€â”€ dataset_paralelo_vX_final.csv # (Opcional) Dataset CSV Tupi-DefiniÃ§Ã£o
â”‚ â””â”€â”€ Brutos/ # Arquivos de log ou erros
â”‚ â””â”€â”€ entradas_com_erro_vX.txt # Logs de erro da extraÃ§Ã£o/pÃ³s-processamento
â”‚ â””â”€â”€ arquivo_pre_filtrado_debug.txt # Texto apÃ³s prÃ©-filtragem (opcional)
â”œâ”€â”€ Python/
â”‚ â””â”€â”€ Notebooks/ # CÃ³digo fonte do processamento e testes
â”‚ â”œâ”€â”€ BookParsing.ipynb # Notebook para parsing do dicionÃ¡rio (provavelmente)
â”‚ â”œâ”€â”€ split_dataset.py # Script para dividir o JSONL em treino/validaÃ§Ã£o
â”‚ â””â”€â”€ inference_test.py # Script/Notebook para testar o modelo fine-tuned
â”œâ”€â”€ tupi-gemma-2b-lora-v8/ # DiretÃ³rio de SAÃDA do Axolotl com adaptadores LoRA <<< AJUSTE O NOME
â”‚ â”œâ”€â”€ adapter_config.json
â”‚ â”œâ”€â”€ adapter_model.safetensors
â”‚ â””â”€â”€ ... (outros arquivos salvos pelo Axolotl/Trainer)
â”œâ”€â”€ .gitattributes
â”œâ”€â”€ config.yaml # Arquivo de configuraÃ§Ã£o do Axolotl para fine-tuning
â””â”€â”€ Readme.md # Este arquivo
```
*(Nota: Substitua `vX` pela versÃ£o final utilizada nos nomes dos arquivos. Ajuste o nome do diretÃ³rio de saÃ­da do Axolotl)*

## 4. Metodologia e Workflow Executado

1.  **ExtraÃ§Ã£o Inicial e PrÃ©-filtragem:** Texto extraÃ­do do PDF e limpeza inicial para remover ruÃ­dos grosseiros.
2.  **Limpeza Robusta:** Uso intensivo de Python e Regex para corrigir erros de OCR e normalizar o texto (etapa crucial e contÃ­nua).
3.  **DivisÃ£o e ExtraÃ§Ã£o Estrutural:** SegmentaÃ§Ã£o do texto em entradas e extraÃ§Ã£o iterativa de campos (`verbete_tupi`, `classe_gramatical`, `definicoes`, etc.) para um arquivo JSON (`dicionario_estruturado_final_vX.json`).
4.  **Processamento da GramÃ¡tica:** SeÃ§Ã£o inicial do dicionÃ¡rio limpa e formatada em `alfabeto_tratado.jsonl`.
5.  **PÃ³s-processamento (Limpeza Final):** AplicaÃ§Ã£o de limpeza adicional nos dados extraÃ­dos do JSON (ex: v8) para refinar verbetes e definiÃ§Ãµes antes de gerar os datasets de treinamento.
6.  **GeraÃ§Ã£o do Dataset de InstruÃ§Ã£o:** ConversÃ£o dos dados limpos (verbete-definiÃ§Ã£o) para o formato JSON Lines (`dataset_instrucao_vX_final.jsonl`), adequado para fine-tuning de instruÃ§Ã£o.
7.  **DivisÃ£o Treino/ValidaÃ§Ã£o:** O dataset `.jsonl` foi dividido (~90%/10%) usando `split_dataset.py` para criar `train_dataset.jsonl` e `validation_dataset.jsonl`.
8.  **ConfiguraÃ§Ã£o do Ambiente de Treinamento:**
    *   UtilizaÃ§Ã£o do **WSL 2 (Windows Subsystem for Linux)** no Windows 11 para melhor compatibilidade das bibliotecas de ML.
    *   CriaÃ§Ã£o de um ambiente **Conda** (`tupi_llm`) com Python 3.10.
    *   InstalaÃ§Ã£o das bibliotecas necessÃ¡rias: `pytorch` (com suporte a CUDA), `transformers`, `datasets`, `accelerate`, `peft`, `trl`, `bitsandbytes`, `sentencepiece`.
    *   InstalaÃ§Ã£o do `build-essential` no WSL para permitir a compilaÃ§Ã£o do `bitsandbytes`.
    *   AutenticaÃ§Ã£o no Hugging Face Hub (`huggingface-cli login`) e aceite dos termos do modelo base.
9.  **Fine-tuning com Axolotl:**
    *   **Modelo Base:** `google/gemma-2b-it` (escolhido por ser pequeno e bom em instruÃ§Ãµes).
    *   **TÃ©cnica:** **QLoRA** (LoRA com quantizaÃ§Ã£o de 4 bits) para viabilizar o treinamento na GPU GTX 1650 (4GB VRAM).
    *   **ConfiguraÃ§Ã£o (`config.yaml`):** Ajustes especÃ­ficos para baixa VRAM, incluindo `sequence_len: 256`, `load_in_4bit: true`, `micro_batch_size: 1`, `gradient_accumulation_steps: 8`, `lora_r: 8`, `gradient_checkpointing: true`.
    *   **ExecuÃ§Ã£o:** Treinamento iniciado usando `accelerate launch -m axolotl.cli.train config.yaml`.
    *   **Resultado:** Adaptadores LoRA salvos em `./tupi-gemma-2b-lora-v8` (ou nome similar).
10. **ConfiguraÃ§Ã£o do Ambiente de InferÃªncia (Colab):**
    *   CriaÃ§Ã£o de um notebook no Google Colab com acesso a GPU (T4).
    *   InstalaÃ§Ã£o das mesmas dependÃªncias (`pip install ...`).
    *   Login no Hugging Face Hub.
11. **Teste/AvaliaÃ§Ã£o Inicial:**
    *   Carregamento do modelo base (`gemma-2b-it`) com quantizaÃ§Ã£o QLoRA.
    *   AplicaÃ§Ã£o dos adaptadores LoRA treinados (baixados do Hub ou localmente).
    *   ExecuÃ§Ã£o de um script (`inference_test.py`) para gerar definiÃ§Ãµes para palavras Tupi e avaliar qualitativamente as respostas.

## 5. Desafios Enfrentados (Atualizado)

*   **Qualidade do OCR:** Continua sendo um desafio de base, exigindo limpeza contÃ­nua.
*   **Estrutura VariÃ¡vel do DicionÃ¡rio:** Dificultou a extraÃ§Ã£o 100% precisa.
*   **Isolamento de Campos:** Separar `verbete_tupi` da classe/definiÃ§Ã£o foi o maior desafio da extraÃ§Ã£o.
*   **LimitaÃ§Ãµes de Hardware (VRAM):** A GTX 1650 exigiu o uso de QLoRA, `sequence_len` baixo (256), `micro_batch_size` 1 e `gradient_checkpointing` para conseguir rodar o fine-tuning do Gemma 2B.
*   **ConfiguraÃ§Ã£o do Ambiente (WSL):** Necessidade de instalar `build-essential` para `bitsandbytes` e configurar corretamente o DNS para acesso ao Hugging Face Hub.
*   **Modelos Gated (Hugging Face):** Necessidade de aceitar termos de uso e autenticar (`huggingface-cli login`).
*   **Carregamento de Adaptadores PEFT:** Encontro de `KeyError` devido a possÃ­veis problemas com `device_map` e quantizaÃ§Ã£o, necessitando ajustes na forma de carregar modelo + adaptadores para inferÃªncia.

## 6. Ferramentas Utilizadas (Atualizado)

*   **Linguagem:** Python 3
*   **Ambiente:** Jupyter Notebook, WSL 2 (Ubuntu), Google Colab
*   **Bibliotecas Principais:** `re`, `json`, `csv`, `os`, `pandas`, `PyPDF2`/`PyMuPDF`, `sklearn`, `torch`, `transformers`, `datasets`, `accelerate`, `peft`, `bitsandbytes`, `trl`, `sentencepiece`
*   **Framework de Fine-tuning:** `axolotl`

## 7. Artefatos Gerados (Principais)

*   `output/Arquivos_treinamento/dicionario_estruturado_final_vX.json`: JSON estruturado (versÃ£o final usada como base).
*   `output/Arquivos_treinamento/alfabeto_tratado.jsonl`: InstruÃ§Ãµes de gramÃ¡tica.
*   `output/Arquivos_treinamento/train_dataset.jsonl`: Dataset de treino para LLM.
*   `output/Arquivos_treinamento/validation_dataset.jsonl`: Dataset de validaÃ§Ã£o para LLM.
*   `tupi-gemma-2b-lora-v8/` (ou similar): **DiretÃ³rio contendo os adaptadores LoRA resultantes do fine-tuning.**
*   `Python/Notebooks/inference_test.py`: Script/Notebook para carregar e testar o modelo fine-tuned.
*   `config.yaml`: Arquivo de configuraÃ§Ã£o do Axolotl usado para o treinamento.

## 8. PrÃ³ximos Passos Imediatos

<<<<<<< HEAD
1.  **AvaliaÃ§Ã£o Qualitativa Detalhada:** Usar o `inference_test.py` para testar uma gama maior de palavras (incluindo as que falharam na extraÃ§Ã£o ou estavam no set de validaÃ§Ã£o) e analisar a qualidade, coerÃªncia e precisÃ£o das definiÃ§Ãµes geradas.
2.  **AnÃ¡lise de Erros:** Identificar os tipos de erros mais comuns que o modelo comete. Ele alucina? Confunde palavras? Gera respostas genÃ©ricas? Falha com termos especÃ­ficos?
3.  **DecisÃ£o Baseada na AvaliaÃ§Ã£o:**
    *   **Se Suficiente para Testes:** Prosseguir com experimentaÃ§Ãµes adicionais ou integraÃ§Ã£o em uma aplicaÃ§Ã£o simples.
    *   **Se Necessita Melhoria (ProvÃ¡vel):**
        *   **Prioridade 1: Melhorar Dados:** Voltar aos scripts de limpeza (`limpar_texto_robusto`) e extraÃ§Ã£o (Passo 5 da v8) para gerar um JSON de entrada *melhor* e *retreinar*. Corrigir os problemas remanescentes no isolamento do verbete e limpeza das definiÃ§Ãµes Ã© crucial.
        *   **Prioridade 2: Ajustar Treinamento:** Experimentar com mais Ã©pocas (`num_epochs: 2` ou `3` no `config.yaml`), ajustar a taxa de aprendizado, ou o rank do LoRA (`lora_r`).
        *   **Prioridade 3: Mais Dados:** Incorporar o `alfabeto_tratado.jsonl` ao treinamento. Buscar outras fontes textuais.

## 9. Como Usar os Artefatos Atuais

*   **`dicionario_estruturado_final_vX.json`:** Fonte para anÃ¡lise e geraÃ§Ã£o de novos formatos de dataset.
*   **`train/validation_dataset.jsonl`:** Usados diretamente para retreinar/continuar o fine-tuning com Axolotl ou TRL.
*   **`tupi-gemma-2b-lora-v8/` (Adaptadores):** Carregar junto com o modelo base `google/gemma-2b-it` para realizar inferÃªncia, usando o `inference_test.py` como exemplo. Compartilhar esta pasta (ou subir para o Hugging Face Hub) permite que outros usem seu modelo fine-tuned.
*   **`config.yaml`:** Documenta os parÃ¢metros usados e pode ser modificado para novos treinamentos.

![Google Colab](https://colab.research.google.com/drive/147oT9B2e-FNg4KGFMFkqhqu-CBYcJaFK?usp=sharing)
# ğŸ“˜ Tupi2LLM - InferÃªncia com Modelo Quantizado e LoRA

Este notebook demonstra como carregar um modelo base de linguagem grande (LLM) com **quantizaÃ§Ã£o 4-bit (QLoRA)** e aplicar adaptadores **LoRA personalizados** para inferÃªncia eficiente em GPUs com memÃ³ria limitada (como T4).

## ğŸ”§ Requisitos
- GPU com suporte a `bfloat16` (ex: T4, A100)
- Conta no [Hugging Face](https://huggingface.co) com token configurado
- Modelos salvos no Hub com arquivos necessÃ¡rios:
  - `config.json` com `model_type`
  - pesos adaptadores (LoRA)

## ğŸ“¦ DependÃªncias
- `transformers`
- `peft`
- `bitsandbytes`
- `torch`

As bibliotecas serÃ£o instaladas automaticamente no inÃ­cio do notebook.

## ğŸ“‘ Etapas do Notebook
1. InstalaÃ§Ã£o das bibliotecas
2. Carregamento do tokenizer
3. ConfiguraÃ§Ã£o de quantizaÃ§Ã£o (QLoRA)
4. Carregamento do modelo base quantizado
5. AplicaÃ§Ã£o dos adaptadores LoRA
6. InferÃªncia pronta! ğŸ¯

## ğŸ“ Identificadores dos Modelos
- **BASE_MODEL_ID**: `meta-llama/Llama-2-7b-hf` (ou equivalente)
- **ADAPTER_HUB_ID**: `peterson047/Tupi2LLM`

> ğŸ’¡ Dica: Os adaptadores devem ser compatÃ­veis com o modelo base!
=======
1.  **ValidaÃ§Ã£o e Refinamento Final:** RevisÃ£o manual e/ou programÃ¡tica do `dicionario_estruturado_final.json` para corrigir erros remanescentes. Idealmente, refinar o(s) notebook(s) de extraÃ§Ã£o em `Python/Notebooks/` se forem encontrados problemas sistemÃ¡ticos.
2.  **GeraÃ§Ã£o dos Datasets de Treinamento Finais:** Criar os arquivos `.txt`, `.csv` ou `.jsonl` definitivos a partir do JSON validado, aplicando a limpeza final durante a geraÃ§Ã£o.
3.  **Fine-tuning do LLM:**
    *   Selecionar um modelo base prÃ©-treinado.
    *   Configurar o ambiente de treinamento (GPU, bibliotecas).
    *   Executar o fine-tuning usando os datasets divididos (treino/validaÃ§Ã£o) e tÃ©cnicas como LoRA/QLoRA.
4.  **AvaliaÃ§Ã£o:** Testar o desempenho do modelo fine-tuned nas tarefas desejadas.
5.  **IteraÃ§Ã£o:** Refinar os datasets ou o processo de fine-tuning com base na avaliaÃ§Ã£o.
>>>>>>> 1baf3ff5714f6e835e004bf46f1b93b60ae19ee6
