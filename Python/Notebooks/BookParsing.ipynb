{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97136aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pypdf2 in c:\\users\\peterson.pereira\\appdata\\local\\anaconda3\\lib\\site-packages (3.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ympy (C:\\Users\\peterson.pereira\\AppData\\Local\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ympy (C:\\Users\\peterson.pereira\\AppData\\Local\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ympy (C:\\Users\\peterson.pereira\\AppData\\Local\\anaconda3\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "%pip install pypdf2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25dc2c91",
   "metadata": {},
   "source": [
    "### Carregando e extraindo dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "797f41e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erro: Arquivo PDF não encontrado em 'C:/Users/peterson.pereira/Documents/GitHub/Tupi2LLM/Carvalho_1987_DicTupiAntigo-Port_OCR.pdf'\n",
      "\n",
      "Não foi possível carregar o texto do PDF.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import random\n",
    "from collections import Counter\n",
    "import unicodedata\n",
    "\n",
    "# --- Tente com PyPDF2 ---\n",
    "try:\n",
    "    import PyPDF2\n",
    "    pdf_reader_type = 'PyPDF2'\n",
    "except ImportError:\n",
    "    print(\"PyPDF2 não encontrado. Tentando PyMuPDF...\")\n",
    "    # --- Tente com PyMuPDF (fitz) ---\n",
    "    try:\n",
    "        import fitz # PyMuPDF\n",
    "        pdf_reader_type = 'PyMuPDF'\n",
    "    except ImportError:\n",
    "        print(\"Erro: Nem PyPDF2 nem PyMuPDF (fitz) estão instalados.\")\n",
    "        print(\"Execute 'pip install PyPDF2' ou 'pip install PyMuPDF'\")\n",
    "        pdf_reader_type = None\n",
    "\n",
    "# --- Configurações ---\n",
    "caminho_arquivo_pdf = 'C:/Users/peterson.pereira/Documents/GitHub/Tupi2LLM/Carvalho_1987_DicTupiAntigo-Port_OCR.pdf' # <<< COLOQUE O CAMINHO CORRETO AQUI\n",
    "\n",
    "caminho_arquivo_txt_extraido = 'C:/Users/peterson.pereira/Documents/GitHub/Tupi2LLM/output/arquivo_bruto_extraido.txt' # Arquivo para salvar o texto bruto extraído\n",
    "caminho_arquivo_limpo_fase1 = 'C:/Users/peterson.pereira/Documents/GitHub/Tupi2LLM/output/arquivo_bruto_limpo.txtfase1.txt'\n",
    "\n",
    "# --- Extrair Texto do PDF ---\n",
    "texto_original = \"\"\n",
    "linhas_originais = []\n",
    "\n",
    "if pdf_reader_type and os.path.exists(caminho_arquivo_pdf):\n",
    "    print(f\"Tentando ler '{caminho_arquivo_pdf}' usando {pdf_reader_type}...\")\n",
    "    try:\n",
    "        if pdf_reader_type == 'PyPDF2':\n",
    "            with open(caminho_arquivo_pdf, 'rb') as pdf_file:\n",
    "                pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "                num_paginas = len(pdf_reader.pages)\n",
    "                print(f\"PDF tem {num_paginas} páginas.\")\n",
    "                for page_num in range(num_paginas):\n",
    "                    if (page_num + 1) % 50 == 0: # Feedback a cada 50 páginas\n",
    "                         print(f\"Processando página {page_num + 1}/{num_paginas}...\")\n",
    "                    page = pdf_reader.pages[page_num]\n",
    "                    texto_original += page.extract_text() + \"\\n\" # Adiciona nova linha entre páginas\n",
    "            print(\"Extração com PyPDF2 concluída.\")\n",
    "\n",
    "        elif pdf_reader_type == 'PyMuPDF':\n",
    "            doc = fitz.open(caminho_arquivo_pdf)\n",
    "            num_paginas = doc.page_count\n",
    "            print(f\"PDF tem {num_paginas} páginas.\")\n",
    "            for page_num in range(num_paginas):\n",
    "                 if (page_num + 1) % 50 == 0: # Feedback a cada 50 páginas\n",
    "                         print(f\"Processando página {page_num + 1}/{num_paginas}...\")\n",
    "                 page = doc.load_page(page_num)\n",
    "                 texto_original += page.get_text(\"text\") + \"\\n\" # Adiciona nova linha entre páginas\n",
    "            doc.close()\n",
    "            print(\"Extração com PyMuPDF concluída.\")\n",
    "\n",
    "        # Salvar o texto bruto extraído (bom para referência)\n",
    "        with open(caminho_arquivo_txt_extraido, 'w', encoding='utf-8') as f_out:\n",
    "             f_out.write(texto_original)\n",
    "        print(f\"Texto bruto extraído salvo em '{caminho_arquivo_txt_extraido}'\")\n",
    "\n",
    "        # Dividir o texto em linhas para as próximas etapas\n",
    "        linhas_originais = texto_original.splitlines()\n",
    "        print(f\"Texto carregado: {len(texto_original)} caracteres, {len(linhas_originais)} linhas (aprox.)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao processar o PDF: {e}\")\n",
    "        texto_original = None\n",
    "        linhas_originais = []\n",
    "\n",
    "elif not pdf_reader_type:\n",
    "    print(\"Nenhuma biblioteca de leitura de PDF funcional encontrada.\")\n",
    "else:\n",
    "     print(f\"Erro: Arquivo PDF não encontrado em '{caminho_arquivo_pdf}'\")\n",
    "\n",
    "# --- Verificação Simples ---\n",
    "if linhas_originais:\n",
    "    print(\"\\n--- Primeiras 5 Linhas Extraídas ---\")\n",
    "    for i, linha in enumerate(linhas_originais[:5]):\n",
    "        print(f\"{i:03d}: {linha}\")\n",
    "else:\n",
    "    print(\"\\nNão foi possível carregar o texto do PDF.\")\n",
    "\n",
    "# (Opcional: Se a extração falhar, você pode tentar copiar manualmente\n",
    "#  o texto de algumas páginas do PDF para um arquivo .txt e usar o\n",
    "#  código da resposta anterior para carregar o .txt)\n",
    "# print(\"\\nAlternativa: Copie o texto manualmente do PDF para um arquivo .txt\")\n",
    "# print(\"E ajuste 'caminho_arquivo_ocr' na célula original para ler esse .txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163dfbf0",
   "metadata": {},
   "source": [
    "### Normalizando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56454907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- PASSO 1: Carregando Texto Bruto ---\n",
      "Texto bruto carregado com sucesso de 'C:/Users/peterson.pereira/Documents/GitHub/Tupi2LLM/output/arquivo_bruto_extraido.txt' usando encoding 'utf-8'.\n",
      "Tamanho: 666298 caracteres.\n",
      "\n",
      "--- PASSO 2: Iniciando Pré-filtragem em Memória ---\n",
      "Pré-filtragem concluída. 10697 linhas removidas.\n",
      "Tamanho do texto filtrado: 655031 caracteres.\n",
      "Texto pré-filtrado (para debug) salvo em 'C:/Users/peterson.pereira/Documents/GitHub/Tupi2LLM/output/arquivo_pre_filtrado_debug.txt'\n",
      "\n",
      "--- PASSO 3: Dividindo Texto em Entradas ---\n",
      "Encontrados 7946 potenciais inícios de entrada no texto filtrado.\n",
      "Dividido em 7946 entradas brutas.\n",
      "\n",
      "--- Exemplo das primeiras 3 entradas brutas (Após Divisão) ---\n",
      "--- Entrada Bruta 1 ---\n",
      "A\n",
      "--------------------\n",
      "--- Entrada Bruta 2 ---\n",
      "AAN-GATU\n",
      "--------------------\n",
      "--- Entrada Bruta 3 ---\n",
      "A\n",
      "--------------------\n",
      "\n",
      "--- PASSO 4: Definindo Função de Limpeza ---\n",
      "Função de limpeza definida.\n",
      "\n",
      "--- PASSO 5: Iniciando Processamento e Extração de 7946 Entradas ---\n",
      "Processando entrada 200/7946...\n",
      "Processando entrada 400/7946...\n",
      "Processando entrada 600/7946...\n",
      "Processando entrada 800/7946...\n",
      "Processando entrada 1000/7946...\n",
      "Processando entrada 1200/7946...\n",
      "Processando entrada 1400/7946...\n",
      "Processando entrada 1600/7946...\n",
      "Processando entrada 1800/7946...\n",
      "Processando entrada 2000/7946...\n",
      "Processando entrada 2200/7946...\n",
      "Processando entrada 2400/7946...\n",
      "Processando entrada 2600/7946...\n",
      "Processando entrada 2800/7946...\n",
      "Processando entrada 3000/7946...\n",
      "Processando entrada 3200/7946...\n",
      "Processando entrada 3400/7946...\n",
      "Processando entrada 3600/7946...\n",
      "Processando entrada 3800/7946...\n",
      "Processando entrada 4000/7946...\n",
      "Processando entrada 4200/7946...\n",
      "Processando entrada 4400/7946...\n",
      "Processando entrada 4600/7946...\n",
      "Processando entrada 4800/7946...\n",
      "Processando entrada 5000/7946...\n",
      "Processando entrada 5200/7946...\n",
      "Processando entrada 5400/7946...\n",
      "Processando entrada 5600/7946...\n",
      "Processando entrada 5800/7946...\n",
      "Processando entrada 6000/7946...\n",
      "Processando entrada 6200/7946...\n",
      "Processando entrada 6400/7946...\n",
      "Processando entrada 6600/7946...\n",
      "Processando entrada 6800/7946...\n",
      "Processando entrada 7000/7946...\n",
      "Processando entrada 7200/7946...\n",
      "Processando entrada 7400/7946...\n",
      "Processando entrada 7600/7946...\n",
      "Processando entrada 7800/7946...\n",
      "\n",
      "Processadas 7946 entradas estruturadas válidas (v2).\n",
      "Encontrados 0 erros durante o processamento.\n",
      "\n",
      "--- PASSO 6: Filtrando Entradas Vazias/Inválidas ---\n",
      "Removidas 7946 entradas durante a filtragem final.\n",
      "\n",
      "--- PASSO 7: Salvando Resultados ---\n",
      "\n",
      "Dados estruturados finais (0 entradas) salvos em 'C:/Users/peterson.pereira/Documents/GitHub/Tupi2LLM/output/dicionario_estruturado_final_v2.json'\n",
      "\n",
      "--- Processamento Concluído ---\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import os\n",
    "import pandas as pd # Import pandas for the validation step later\n",
    "import random     # Import random for sampling\n",
    "\n",
    "# ==============================================================================\n",
    "# --- Configurações ---\n",
    "# ==============================================================================\n",
    "\n",
    "# Caminho para o SEU arquivo bruto extraido do PDF\n",
    "# AJUSTE ESTE CAMINHO CONFORME NECESSÁRIO\n",
    "caminho_arquivo_bruto = 'C:/Users/peterson.pereira/Documents/GitHub/Tupi2LLM/output/arquivo_bruto_extraido.txt'\n",
    "\n",
    "# Caminho para salvar o texto pré-filtrado (OPCIONAL, para debug)\n",
    "# Se não quiser salvar, pode deixar comentado ou None\n",
    "caminho_arquivo_pre_filtrado_opcional = 'C:/Users/peterson.pereira/Documents/GitHub/Tupi2LLM/output/arquivo_pre_filtrado_debug.txt'\n",
    "# caminho_arquivo_pre_filtrado_opcional = None # Descomente esta linha para não salvar\n",
    "\n",
    "# Caminho para salvar os dados estruturados finais em JSON - SAÍDA PRINCIPAL\n",
    "# Usando v2 para diferenciar desta versão do script\n",
    "caminho_arquivo_json = 'C:/Users/peterson.pereira/Documents/GitHub/Tupi2LLM/output/dicionario_estruturado_final_v2.json'\n",
    "\n",
    "# Caminho para salvar informações sobre entradas que causaram erros\n",
    "caminho_arquivo_erros = 'C:/Users/peterson.pereira/Documents/GitHub/Tupi2LLM/output/entradas_com_erro_v2.txt'\n",
    "\n",
    "# ==============================================================================\n",
    "# --- PASSO 1: Carregar o Texto Bruto ---\n",
    "# ==============================================================================\n",
    "print(\"--- PASSO 1: Carregando Texto Bruto ---\")\n",
    "texto_completo_bruto = None\n",
    "encodings_to_try = ['utf-8', 'latin-1', 'cp1252'] # Lista de codificações comuns\n",
    "for enc in encodings_to_try:\n",
    "    try:\n",
    "        with open(caminho_arquivo_bruto, 'r', encoding=enc) as f:\n",
    "            texto_completo_bruto = f.read()\n",
    "        print(f\"Texto bruto carregado com sucesso de '{caminho_arquivo_bruto}' usando encoding '{enc}'.\")\n",
    "        print(f\"Tamanho: {len(texto_completo_bruto)} caracteres.\")\n",
    "        break # Sai do loop se carregar com sucesso\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Erro Crítico: Arquivo bruto não encontrado em '{caminho_arquivo_bruto}'\")\n",
    "        texto_completo_bruto = None\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"Falha ao ler com encoding '{enc}': {e}\")\n",
    "        continue # Tenta o próximo encoding\n",
    "\n",
    "if texto_completo_bruto is None:\n",
    "    print(\"Erro Crítico: Não foi possível carregar o arquivo bruto. Verifique o caminho e a codificação.\")\n",
    "    # Encerra o script ou lida com o erro como preferir\n",
    "    exit()\n",
    "\n",
    "# ==============================================================================\n",
    "# --- PASSO 2: Pré-filtragem em Memória ---\n",
    "# ==============================================================================\n",
    "print(\"\\n--- PASSO 2: Iniciando Pré-filtragem em Memória ---\")\n",
    "texto_filtrado_memoria = \"\"\n",
    "linhas_removidas_count = 0\n",
    "if texto_completo_bruto:\n",
    "    linhas_brutas = texto_completo_bruto.splitlines()\n",
    "    linhas_filtradas = []\n",
    "    num_linhas_brutas = len(linhas_brutas)\n",
    "\n",
    "    for i, linha in enumerate(linhas_brutas):\n",
    "        linha_strip = linha.strip()\n",
    "\n",
    "        # Remover linhas vazias\n",
    "        if not linha_strip:\n",
    "            linhas_removidas_count += 1\n",
    "            continue\n",
    "        # Remover linhas que são apenas números (provavelmente páginas)\n",
    "        if re.fullmatch(r'\\d+', linha_strip):\n",
    "            linhas_removidas_count += 1\n",
    "            continue\n",
    "        # Remover linhas separadoras comuns (estrelas, 'k', traços repetidos)\n",
    "        if re.fullmatch(r'[\\*k -]+', linha_strip) and len(linha_strip) > 3:\n",
    "             linhas_removidas_count += 1\n",
    "             continue\n",
    "        # Remover running headers específicos (Ex: VERBETE / VERBETE?) - AJUSTE SE NECESSÁRIO!\n",
    "        # Esta regex pode precisar ser adaptada ao padrão exato do seu PDF\n",
    "        # A heurística verifica se a linha se parece com um header e se está isolada (linhas acima/abaixo curtas ou vazias)\n",
    "        if i > 0 and i < num_linhas_brutas - 1: # Evita checar primeira/última linha\n",
    "             if re.fullmatch(r'[A-ZÁÉÍÓÚÇÑŸ\\-]+\\s*(?:/\\s*[A-ZÁÉÍÓÚÇÑŸ\\-]+\\??|-)\\s*', linha_strip, re.IGNORECASE):\n",
    "                 # Verifica contexto: linha acima ou abaixo vazia/curta aumenta chance de ser header\n",
    "                 linha_anterior_vazia = not linhas_brutas[i-1].strip()\n",
    "                 linha_posterior_vazia = not linhas_brutas[i+1].strip()\n",
    "                 if linha_anterior_vazia or linha_posterior_vazia or len(linhas_brutas[i-1]) < 15 or len(linhas_brutas[i+1]) < 15:\n",
    "                    # print(f\"Removendo linha de header provável: '{linha_strip}'\") # Descomentar para debug\n",
    "                    linhas_removidas_count += 1\n",
    "                    continue\n",
    "        # Remover linhas de referência da biblioteca digital (ajuste se necessário)\n",
    "        if \"Biblioteca Digital Curt Nimuendajú\" in linha or \"http://www.etnolinguistica.org\" in linha:\n",
    "            linhas_removidas_count += 1\n",
    "            continue\n",
    "\n",
    "        # Se passou por todos os filtros, mantém a linha original\n",
    "        linhas_filtradas.append(linha)\n",
    "\n",
    "    texto_filtrado_memoria = \"\\n\".join(linhas_filtradas)\n",
    "    print(f\"Pré-filtragem concluída. {linhas_removidas_count} linhas removidas.\")\n",
    "    print(f\"Tamanho do texto filtrado: {len(texto_filtrado_memoria)} caracteres.\")\n",
    "\n",
    "    # --- OPCIONAL: Salvar o texto pré-filtrado para debug ---\n",
    "    if caminho_arquivo_pre_filtrado_opcional:\n",
    "        try:\n",
    "            with open(caminho_arquivo_pre_filtrado_opcional, 'w', encoding='utf-8') as f_out:\n",
    "                f_out.write(texto_filtrado_memoria)\n",
    "            print(f\"Texto pré-filtrado (para debug) salvo em '{caminho_arquivo_pre_filtrado_opcional}'\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao salvar arquivo pré-filtrado opcional: {e}\")\n",
    "    # ---------------------------------------------------------\n",
    "else:\n",
    "    print(\"Texto bruto não carregado, pulando etapas subsequentes.\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# --- PASSO 3: Dividir o Texto Filtrado em Entradas ---\n",
    "# ==============================================================================\n",
    "print(\"\\n--- PASSO 3: Dividindo Texto em Entradas ---\")\n",
    "entradas_brutas = []\n",
    "if texto_filtrado_memoria:\n",
    "    # Regex para identificar inícios de entrada (mais robusta)\n",
    "    # Procura linhas que começam com pouca indentação E:\n",
    "    #   Opção 1: Opcional '— ' seguido por Maiúscula ou Acentuada Maiúscula\n",
    "    #   Opção 2: Apenas uma Letra Maiúscula (marcador de seção como A, B, C)\n",
    "    regex_inicio_entrada = re.compile(r\"^(?:\\s{0,3}(?:—\\s)?(?:[A-ZÁÉÍÓÚÇÑŸ]|[ÁÉÍÓÚ]\\b))|^(?:\\s{0,3}[A-Z]\\s*)$\", re.MULTILINE)\n",
    "\n",
    "    indices_inicio = [m.start() for m in regex_inicio_entrada.finditer(texto_filtrado_memoria)]\n",
    "\n",
    "    if indices_inicio:\n",
    "        print(f\"Encontrados {len(indices_inicio)} potenciais inícios de entrada no texto filtrado.\")\n",
    "        for i in range(len(indices_inicio)):\n",
    "            start = indices_inicio[i]\n",
    "            # O fim é o início da próxima entrada, ou o fim do texto\n",
    "            end = indices_inicio[i+1] if (i+1) < len(indices_inicio) else len(texto_filtrado_memoria)\n",
    "            bloco = texto_filtrado_memoria[start:end].strip()\n",
    "            if bloco: # Adiciona apenas se não for vazio\n",
    "                entradas_brutas.append(bloco)\n",
    "\n",
    "        print(f\"Dividido em {len(entradas_brutas)} entradas brutas.\")\n",
    "\n",
    "        # --- DEBUG: Mostrar algumas entradas brutas ---\n",
    "        if entradas_brutas:\n",
    "            print(\"\\n--- Exemplo das primeiras 3 entradas brutas (Após Divisão) ---\")\n",
    "            for i, entrada in enumerate(entradas_brutas[:3]):\n",
    "                print(f\"--- Entrada Bruta {i+1} ---\")\n",
    "                print(entrada) # Mostra a entrada completa para análise\n",
    "                print(\"-\" * 20)\n",
    "        # ---------------------------------------------\n",
    "    else:\n",
    "        print(\"Nenhum início de entrada encontrado com o padrão atual no texto filtrado.\")\n",
    "else:\n",
    "    print(\"Texto filtrado em memória está vazio.\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# --- PASSO 4: Função de Limpeza Robusta ---\n",
    "# ==============================================================================\n",
    "print(\"\\n--- PASSO 4: Definindo Função de Limpeza ---\")\n",
    "\n",
    "def limpar_texto_robusto(texto):\n",
    "    \"\"\"\n",
    "    Aplica um conjunto extensivo de regras de limpeza e normalização ao texto.\n",
    "    !!!!! ESTA FUNÇÃO PRECISA SER MUITO EXPANDIDA COM REGRAS ESPECÍFICAS !!!!!\n",
    "    !!!!! BASEADAS NOS ERROS REAIS DO SEU ARQUIVO OCR.             !!!!!\n",
    "    \"\"\"\n",
    "    if not texto: return \"\"\n",
    "\n",
    "    # 1. Normalização básica inicial (ex: múltiplos espaços)\n",
    "    texto = re.sub(r'\\s+', ' ', texto).strip()\n",
    "\n",
    "    # 2. Correções específicas de OCR e Inconsistências (ADICIONE MUITAS MAIS!)\n",
    "    #    Use um dicionário para facilitar a manutenção. Chaves são os erros, valores as correções.\n",
    "    #    Seja o mais específico possível para evitar substituições indesejadas.\n",
    "    correcoes = {\n",
    "        # Exemplos de erros comuns (Adapte e EXPANDA MUITO!)\n",
    "         'Prefi.xo': 'Prefixo', 'fiubstantivo': 'Substantivo', 'Substarítivo': 'Substantivo',\n",
    "         'S)ibsta\\'ntivo': 'Substantivo', 'Substantlvo': 'Substantivo', 'Sübstantivo': 'Substantivo',\n",
    "         'Adj^fcivo': 'Adjetivo', 'A_d jetivo':'Adjetivo', 'Mjetivao': 'Adjetivo',\n",
    "         'Partlculas': 'Partículas', 'Patticulas': 'Partículas', 'Partícúla': 'Partícula', 'Particlpio':'Particípio',\n",
    "         'Advérbiorse': 'Advérbio: se','Adverbio:': 'Advérbio:', 'Adverbio': 'Advérbio',\n",
    "         'Verbotransitivo': 'Verbo transitivo', 'Verbo intransitivo:o': 'Verbo intransitivo:',\n",
    "         'Verbo paredioativo': 'Verbo predicativo',\n",
    "         'observagoes': 'observações', 'índío': 'índio', 'nao': 'não', 'Nao': 'Não', 'náo': 'não',\n",
    "         'fungáo': 'função', 'conjugagao': 'conjugação', 'Cabega': 'Cabeça', 'cabeqa': 'cabeça',\n",
    "         'composigao': 'composição', 'posipoes': 'posições', 'só ha': 'só há', 'ha': 'há',\n",
    "         'intradutlvel': 'intradutível', 'próxi mo': 'próximo', 'aqáo': 'ação', 'signíficado': 'significado',\n",
    "         'á': 'ã', 'Á': 'Ã', '§': 'ç', '£': 'S', # Suposições para símbolos estranhos\n",
    "         ' 9 ': ' ç ', '9 ': 'ç ', ' 9': ' ç', '(t, 9 )':'(t, ç)', # Correção mais agressiva para 9\n",
    "         'ü': 'u', 'Ü': 'U', # Normalizar trema se desnecessário\n",
    "         # 'y': 'y', # Manter Y com trema se for caractere válido no Tupi Antigo do dicionário\n",
    "         ' l ': ' i ', # l isolado vira i? Cautela.\n",
    "         'permiss^L': 'permissivo', '^fruto':'1 fruto', ' z ': ' 2 ', 'zGlande':'2 Glande','^contradizer':'1 contradizer', # Tenta numerar\n",
    "         'si nal': 'sinal', 'planta (desenho)': 'planta (desenho)',\n",
    "         ' prepoderante':' preponderante', ' oontrário':' contrário', ' riiinca':' nunca',\n",
    "         ' afcsolutamente':' absolutamente', ' a/grupadas':' agrupadas', ' sicj':' sic ',\n",
    "         'multidao': 'multidão', 'duplicagao': 'duplicação', 'duplicaqao': 'duplicação',\n",
    "         'procedéncia': 'procedência', 'género': 'gênero', 'famllia': 'família', 'séxo': 'sexo',\n",
    "         'mé' : 'mãe', 'Índío': 'índio', 'fíubstantivo':'Substantivo','fago':'fago','recordagao':'recordação',\n",
    "         'traduqáo': 'tradução', 'conjungáp': 'conjunção', 'Erigado':'Eriçado',\n",
    "         'diminutl:vo':'diminutivo','répleto':'repleto','pélos':'pelos','respiraqao':'respiração',\n",
    "         'fiaqao':'fiação','recepgáo':'recepção','AgÉ-AgEMA':'AGÉ-AGEMA', 'las.':'1as.',\n",
    "         'preposi^ gao':'preposição', 'pessbal':'pessoal','prepos_i gáo':'preposição', 'Adáo':'Adão',\n",
    "         'abundáncia':'abundância','Preposdgáo':'Preposição','desen^Dlvem':'desenvolvem',\n",
    "         'mao':'mão','cansago':'cansaço','conjugáo':'conjunção', 'oragáo':'oração', 'prSprio':'próprio',\n",
    "         'oraqoes':'orações','ünica':'única','preposiqoes':'preposições','preposiqáo':'preposição',\n",
    "         'entá o':'então','naqu_i':'naquilo','interroga tiva':'interrogativa', 'encru2ilhada':'encruzilhada',\n",
    "         'fungoes':'funções','sé':'só','agao':'ação','posiqáo':'posição','explrlito':'espírito',\n",
    "         'relaqao':'relação', 'engros- sar':'engrossar','solugáo':'solução',\n",
    "         'ÍABA': 'ABA', 'f ': '† ', '+ ': '‡ ', # Usar símbolos distintos ou remover depois\n",
    "         'tupi) . É': 'tupi). É', # Corrigir espaço antes de ponto final\n",
    "         '(XE).': '(XE)', '(T-).': '(T-)', '(Q-).': '(Q-)', # Remover ponto após marcadores\n",
    "         # ... Adicione CENTENAS de regras aqui conforme identificar erros ...\n",
    "         'excrémenlo': 'excremento', 'portu- gués': 'português', 'porem': 'porém',\n",
    "         'sé': 'se', 'ré': 'ré', 'é': 'é', 'í': 'i', 'Ó': 'ó', 'ó': 'ó', # Normalizar acentos se necessário\n",
    "         'qüera':'güera', 'sübstantivo':'substantivo', 'egé':'eçé', # Exemplos de normalização ortográfica Tupi? (Verificar!)\n",
    "    }\n",
    "\n",
    "    texto_processado = texto # Começa com o texto já com espaços normalizados\n",
    "\n",
    "    # Aplica as correções específicas\n",
    "    # Usar replace simples pode ser mais rápido para muitas regras\n",
    "    for erro, correcao in correcoes.items():\n",
    "        texto_processado = texto_processado.replace(erro, correcao)\n",
    "\n",
    "    # 3. Correções com Regex (para padrões mais complexos)\n",
    "    # Ex: Corrigir '9' que sobrou dentro de palavras (se necessário e seguro)\n",
    "    # texto_processado = re.sub(r'(\\w)9(\\w)', r'\\1ç\\2', texto_processado)\n",
    "    # Ex: Normalizar hífens (garantir espaço antes/depois ou remover espaços)\n",
    "    texto_processado = re.sub(r'\\s+-\\s+', '-', texto_processado) # Junta hífens soltos\n",
    "    # Ex: Corrigir espaços antes/depois de parênteses/colchetes/chaves\n",
    "    texto_processado = re.sub(r'\\s*([({\\[])\\s*', r' \\1', texto_processado) # Espaço ANTES de abrir\n",
    "    texto_processado = re.sub(r'\\s*([)}\\]])\\s*', r'\\1 ', texto_processado) # Espaço DEPOIS de fechar\n",
    "\n",
    "    # 4. Normalização final de Espaçamento e Pontuação\n",
    "    texto_processado = re.sub(r' +', ' ', texto_processado).strip() # Remove múltiplos espaços\n",
    "    # Garantir espaço APÓS pontuação comum (exceto fim de string)\n",
    "    texto_processado = re.sub(r'([.,;:?!])([a-zA-ZÀ-ú0-9])', r'\\1 \\2', texto_processado)\n",
    "    # Remover espaço ANTES de pontuação comum\n",
    "    texto_processado = re.sub(r'\\s([.,;:?!])', r'\\1', texto_processado)\n",
    "    # Remover espaço antes de dois pontos, se seguido por espaço (Ex: \"Substantivo : algo\")\n",
    "    texto_processado = re.sub(r'\\s+:\\s+', ': ', texto_processado)\n",
    "    # Garantir que ':' no fim de classes gramaticais tenha espaço depois se seguido por letra\n",
    "    texto_processado = re.sub(r'([a-zA-Z]):([a-zA-Z])', r'\\1: \\2', texto_processado)\n",
    "\n",
    "    return texto_processado.strip()\n",
    "\n",
    "print(\"Função de limpeza definida.\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# --- PASSO 5: Processamento e Extração Refinados ---\n",
    "# ==============================================================================\n",
    "dados_estruturados = []\n",
    "entradas_com_erro = []\n",
    "\n",
    "# Lista expandida de possíveis inícios de classes gramaticais (terminando com :)\n",
    "# Adapte esta lista conforme as classes EXATAS do seu dicionário após limpeza\n",
    "CLASSES_GRAMATICAIS = sorted([ # Ordenar facilita a leitura\n",
    "    'Prefixo', 'Sufixo verbal', 'Sufixo', 'Afixo', 'Substantivo', 'Gerúndio',\n",
    "    'Verbo transitivo irregular', 'Verbo intransitivo irregular', # Mais específicos primeiro\n",
    "    'Verbo transitivo relativo', 'Verbo transitivo', 'Verbo intransitivo',\n",
    "    'Verbo predicativo', 'Verbo factivo', 'Verbo pronominal', 'Verbo reflexivo',\n",
    "    'Verbo iterativo', 'Verbo defectivo', 'Verbo', # Genérico\n",
    "    'Vocativo', 'Demonstrativo', 'Partículas agrupadas', 'Partícula', 'Advérbio',\n",
    "    'Adjetivo', 'Pronome interrogativo', 'Pronome pessoal', 'Pronome relativo',\n",
    "    'Pronome indefinido', 'Pronome possessivo', 'Pronome', # Genérico\n",
    "    'Numeral', 'Interjeigáo', 'Conjunqáo', 'Preposiqáo', 'Locuqáo prepositiva',\n",
    "    'Locuqáo adverbial', 'Locuqáo substantiva', 'Particípio', 'Ictiologia',\n",
    "    'Ornitologia', 'Zoologia', 'Botánica', 'Entomologia', 'Conquiliologia',\n",
    "    'Crustaceologia', 'Astronomia', 'Mineralogia', 'Mitologia', 'Malacologia', # Adicionar mais se houver\n",
    "    'Fitologia' # Exemplo adicional\n",
    "], key=len, reverse=True) # Processar nomes mais longos primeiro para evitar matches parciais\n",
    "\n",
    "# Regex para encontrar uma dessas classes (case-insensitive) seguida por ':' ou fim de string\n",
    "regex_classe = re.compile(r'(\\b(?:' + '|'.join(re.escape(c) for c in CLASSES_GRAMATICAIS) + r')(?:\\s+[a-z]+)*)(?::|\\s*$)', re.IGNORECASE)\n",
    "\n",
    "if 'entradas_brutas' in locals() and entradas_brutas:\n",
    "    print(f\"\\n--- PASSO 5: Iniciando Processamento e Extração de {len(entradas_brutas)} Entradas ---\")\n",
    "    for i, entrada_bruta in enumerate(entradas_brutas):\n",
    "        if (i + 1) % 200 == 0:\n",
    "            print(f\"Processando entrada {i+1}/{len(entradas_brutas)}...\")\n",
    "\n",
    "        try:\n",
    "            entrada_limpa = limpar_texto_robusto(entrada_bruta) # Limpa a entrada inteira primeiro\n",
    "            linhas_entrada = entrada_limpa.splitlines() # Re-split após limpeza para pegar a primeira linha\n",
    "            if not linhas_entrada: linhas_limpas = [entrada_limpa] # Caso a limpeza junte tudo\n",
    "\n",
    "            primeira_linha = linhas_limpas[0].strip()\n",
    "            corpo_linhas_texto = \" \".join(linhas_limpas[1:]).strip() # Junta o resto com espaços\n",
    "\n",
    "            verbete_tupi = \"\"\n",
    "            marcadores = None\n",
    "            classe_gramatical = None\n",
    "            texto_para_definicoes = \"\"\n",
    "            linha_processada = primeira_linha # Linha para extrair verbete/classe\n",
    "\n",
    "            # 1. Tenta extrair marcador (primeiro para simplificar a linha)\n",
    "            match_marcador = re.search(r'(\\s*[({\\[].*?[)}\\]])', linha_processada)\n",
    "            if match_marcador:\n",
    "                marcadores = match_marcador.group(1).strip()\n",
    "                linha_processada = (linha_processada[:match_marcador.start()] + linha_processada[match_marcador.end():]).strip()\n",
    "\n",
    "            # 2. Tenta encontrar a CLASSE GRAMATICAL na linha processada\n",
    "            match_classe_obj = regex_classe.search(linha_processada)\n",
    "            pos_classe = match_classe_obj.start() if match_classe_obj else -1\n",
    "\n",
    "            if pos_classe != -1:\n",
    "                 # Classe encontrada na primeira linha\n",
    "                 verbete_tupi = linha_processada[:pos_classe].strip()\n",
    "                 classe_achada = match_classe_obj.group(1).strip()\n",
    "                 # Verifica se a classe achada está na nossa lista (evita falsos positivos)\n",
    "                 if any(classe_achada.lower() == c.lower() for c in CLASSES_GRAMATICAIS):\n",
    "                      classe_gramatical = classe_achada\n",
    "                      texto_para_definicoes = linha_processada[match_classe_obj.end():].strip() + \" \" + corpo_linhas_texto\n",
    "                 else: # Falso positivo, tratar como parte do verbete ou definição\n",
    "                      verbete_tupi = linha_processada.strip() # Assume verbete é tudo\n",
    "                      texto_para_definicoes = corpo_linhas_texto\n",
    "            else:\n",
    "                 # Nenhuma classe na primeira linha, verbete é a linha toda (limpa)\n",
    "                 verbete_tupi = linha_processada.strip()\n",
    "                 texto_para_definicoes = corpo_linhas_texto # Definição começa nas linhas seguintes\n",
    "                 # Tenta achar classe no INÍCIO do corpo\n",
    "                 match_classe_corpo = regex_classe.match(texto_para_definicoes)\n",
    "                 if match_classe_corpo:\n",
    "                      classe_achada_corpo = match_classe_corpo.group(1).strip()\n",
    "                      if any(classe_achada_corpo.lower() == c.lower() for c in CLASSES_GRAMATICAIS):\n",
    "                          classe_gramatical = classe_achada_corpo\n",
    "                          texto_para_definicoes = texto_para_definicoes[match_classe_corpo.end():].strip()\n",
    "\n",
    "            # 3. Limpeza final do verbete\n",
    "            verbete_tupi = re.sub(r'[:.—?]+\\s*$', '', verbete_tupi).strip() # Remove pontuação final\n",
    "            if verbete_tupi.startswith('— '): verbete_tupi = verbete_tupi[2:]\n",
    "            # Evitar remover traço se for parte do nome como AAN-GATU\n",
    "            # if verbete_tupi.endswith(' —') and not '-' in verbete_tupi[:-2]: verbete_tupi = verbete_tupi[:-2]\n",
    "\n",
    "            # Normalizar classe gramatical (ex: colocar primeira letra maiúscula)\n",
    "            if classe_gramatical:\n",
    "                 classe_gramatical = classe_gramatical.strip().capitalize()\n",
    "\n",
    "\n",
    "            # 4. Extrair Definições, Exemplos, Referências do texto_para_definicoes\n",
    "            definicoes = []\n",
    "            exemplos = []\n",
    "            referencias = []\n",
    "\n",
    "            # Dividir o texto por marcadores de início (números, EX:, =, †, ‡)\n",
    "            # Usando lookahead positivo (?=...) para manter os delimitadores no início das partes\n",
    "            # O padrão agora é mais explícito\n",
    "            partes_corpo = re.split(r'(?=\\s*(?:[1-9]+\\.|†|‡|\\^|z\\b|EX:| = ))', texto_para_definicoes)\n",
    "\n",
    "            item_atual = {\"tipo\": \"definicao\", \"texto\": \"\"} # Começa assumindo definição\n",
    "\n",
    "            # Processa a primeira parte separadamente se não começar com marcador\n",
    "            if partes_corpo and partes_corpo[0].strip() and not re.match(r'^\\s*(?:[1-9]+\\.|†|‡|\\^|z\\b|EX:| = )', partes_corpo[0]):\n",
    "                 item_atual[\"texto\"] = partes_corpo[0].strip()\n",
    "                 partes_corpo = partes_corpo[1:] # Processa o resto a partir do segundo elemento\n",
    "\n",
    "            for parte in partes_corpo:\n",
    "                parte = parte.strip()\n",
    "                if not parte: continue\n",
    "\n",
    "                match_ex = re.match(r'^\\s*EX:\\s*(.*)', parte, re.IGNORECASE)\n",
    "                match_ref = re.match(r'^\\s*=\\s*(.*)', parte)\n",
    "                match_num = re.match(r'^\\s*([1-9]+\\.|†|‡|\\^|z\\b)\\s*(.*)', parte) # Captura o marcador e o texto\n",
    "\n",
    "                tipo_novo_item = None\n",
    "                texto_novo_item = \"\"\n",
    "\n",
    "                if match_ex:\n",
    "                     tipo_novo_item = \"exemplo\"\n",
    "                     texto_novo_item = match_ex.group(1).strip()\n",
    "                elif match_ref:\n",
    "                     tipo_novo_item = \"referencia\"\n",
    "                     texto_novo_item = match_ref.group(1).strip()\n",
    "                elif match_num:\n",
    "                     tipo_novo_item = \"definicao\"\n",
    "                     texto_novo_item = match_num.group(2).strip() # Pega só o texto após o marcador\n",
    "                else:\n",
    "                     # Se a parte não começa com marcador conhecido, é continuação do item anterior\n",
    "                     item_atual[\"texto\"] += \" \" + parte\n",
    "                     continue # Pula para a próxima parte sem salvar/resetar item_atual\n",
    "\n",
    "                # Se encontrou um novo marcador (tipo_novo_item não é None), salva o item anterior (se tiver texto)\n",
    "                if item_atual[\"texto\"]:\n",
    "                     if item_atual[\"tipo\"] == \"definicao\": definicoes.append(item_atual[\"texto\"])\n",
    "                     elif item_atual[\"tipo\"] == \"exemplo\": exemplos.append(item_atual[\"texto\"])\n",
    "                     elif item_atual[\"tipo\"] == \"referencia\": referencias.append(item_atual[\"texto\"])\n",
    "\n",
    "                # Inicia o novo item\n",
    "                item_atual = {\"tipo\": tipo_novo_item, \"texto\": texto_novo_item}\n",
    "\n",
    "\n",
    "            # Salva o último item acumulado\n",
    "            if item_atual[\"texto\"]:\n",
    "                 if item_atual[\"tipo\"] == \"definicao\": definicoes.append(item_atual[\"texto\"])\n",
    "                 elif item_atual[\"tipo\"] == \"exemplo\": exemplos.append(item_atual[\"texto\"])\n",
    "                 elif item_atual[\"tipo\"] == \"referencia\": referencias.append(item_atual[\"texto\"])\n",
    "\n",
    "\n",
    "            # 5. Armazenar dados estruturados (com filtro final)\n",
    "            if verbete_tupi or definicoes or exemplos or referencias:\n",
    "                 # Filtra entradas que são apenas marcadores de seção (A, B, C...)\n",
    "                 # E entradas onde o verbete ficou vazio e não há outros dados\n",
    "                 if not re.fullmatch(r'[A-Z]\\s*$', primeira_linha.strip()) and verbete_tupi:\n",
    "                    dados_estruturados.append({\n",
    "                        'id': len(dados_estruturados),\n",
    "                        'verbete_original_linha': primeira_linha,\n",
    "                        'verbete_tupi': verbete_tupi,\n",
    "                        'marcadores': marcadores,\n",
    "                        'classe_gramatical': classe_gramatical,\n",
    "                        'definicoes': definicoes,\n",
    "                        'exemplos': exemplos,\n",
    "                        'referencias': referencias,\n",
    "                    })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\n!!!!!! Erro Crítico ao processar entrada bruta {i} !!!!!!\")\n",
    "            print(f\"Erro: {e}\")\n",
    "            # Limita o tamanho do texto guardado no log de erro\n",
    "            texto_erro = entrada_bruta[:1000] + \"...\" if len(entrada_bruta) > 1000 else entrada_bruta\n",
    "            entradas_com_erro.append({\"id_bruta\": i, \"erro\": str(e), \"texto\": texto_erro})\n",
    "            # Considere parar o script aqui em caso de erro inesperado, ou continue com cuidado\n",
    "            # raise e # Descomente para parar o script no primeiro erro grave\n",
    "\n",
    "    print(f\"\\nProcessadas {len(dados_estruturados)} entradas estruturadas válidas (v2).\")\n",
    "    print(f\"Encontrados {len(entradas_com_erro)} erros durante o processamento.\")\n",
    "\n",
    "    # --- PASSO 6: Filtragem Final (Opcional, mas recomendado) ---\n",
    "    print(\"\\n--- PASSO 6: Filtrando Entradas Vazias/Inválidas ---\")\n",
    "    dados_filtrados_final = [\n",
    "        entry for entry in dados_estruturados\n",
    "        if entry.get('verbete_tupi') and \\\n",
    "           (entry.get('classe_gramatical') or entry.get('definicoes') or entry.get('exemplos') or entry.get('referencias'))\n",
    "    ]\n",
    "    num_removidos_filtragem = len(dados_estruturados) - len(dados_filtrados_final)\n",
    "    print(f\"Removidas {num_removidos_filtragem} entradas durante a filtragem final.\")\n",
    "    dados_estruturados = dados_filtrados_final # Usa a lista filtrada\n",
    "\n",
    "    # --- PASSO 7: Salvar Resultados ---\n",
    "    print(\"\\n--- PASSO 7: Salvando Resultados ---\")\n",
    "    # ----- Salvar os dados estruturados em JSON -----\n",
    "    try:\n",
    "        output_dir_json = os.path.dirname(caminho_arquivo_json)\n",
    "        if output_dir_json and not os.path.exists(output_dir_json):\n",
    "            print(f\"Criando diretório de saída para JSON: {output_dir_json}\")\n",
    "            os.makedirs(output_dir_json)\n",
    "        with open(caminho_arquivo_json, 'w', encoding='utf-8') as f_json:\n",
    "            json.dump(dados_estruturados, f_json, ensure_ascii=False, indent=2)\n",
    "        print(f\"\\nDados estruturados finais ({len(dados_estruturados)} entradas) salvos em '{caminho_arquivo_json}'\")\n",
    "\n",
    "        # Mostrar exemplo das 5 primeiras entradas estruturadas\n",
    "        if dados_estruturados:\n",
    "            print(\"\\n--- Exemplo das Primeiras 5 Entradas Estruturadas Finais (JSON v2) ---\")\n",
    "            for k in range(min(5, len(dados_estruturados))):\n",
    "                 print(f\"--- Entrada ID {dados_estruturados[k]['id']} ---\")\n",
    "                 print(json.dumps(dados_estruturados[k], ensure_ascii=False, indent=2))\n",
    "                 print(\"-\" * 20)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao salvar o arquivo JSON final: {e}\")\n",
    "\n",
    "    # ----- Salvar entradas com erro (se houver) -----\n",
    "    if entradas_com_erro:\n",
    "        try:\n",
    "            output_dir_erro = os.path.dirname(caminho_arquivo_erros)\n",
    "            if output_dir_erro and not os.path.exists(output_dir_erro):\n",
    "                print(f\"Criando diretório de saída para Erros: {output_dir_erro}\")\n",
    "                os.makedirs(output_dir_erro)\n",
    "            with open(caminho_arquivo_erros, 'w', encoding='utf-8') as f_err:\n",
    "                 json.dump(entradas_com_erro, f_err, ensure_ascii=False, indent=2) # Salva como JSON para facilitar\n",
    "            print(f\"Informações de {len(entradas_com_erro)} erro(s) salvas em '{caminho_arquivo_erros}'\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao salvar arquivo de erros: {e}\")\n",
    "\n",
    "elif not 'entradas_brutas' in locals() or not entradas_brutas:\n",
    "     print(\"Não há entradas brutas para processar. Verifique os Passos 1, 2 e 3.\")\n",
    "\n",
    "print(\"\\n--- Processamento Concluído ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ffa34f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processadas 1485 entradas estruturadas.\n",
      "\n",
      "Dados estruturados salvos em 'C:/Users/peterson.pereira/Documents/GitHub/Tupi2LLM/output/dicionario_estruturado.json'\n",
      "\n",
      "--- Exemplo da Primeira Entrada Estruturada (JSON) ---\n",
      "{\n",
      "  \"id\": 0,\n",
      "  \"verbete_original\": \"A\",\n",
      "  \"verbete_tupi\": \"A\",\n",
      "  \"marcadores\": null,\n",
      "  \"classe_gramatical\": null,\n",
      "  \"definicoes\": [],\n",
      "  \"exemplos\": [],\n",
      "  \"referencias\": [],\n",
      "  \"texto_completo_entrada\": \"A\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def limpar_texto(texto):\n",
    "    \"\"\"Função auxiliar para limpeza básica rápida\"\"\"\n",
    "    if not texto: return \"\"\n",
    "    # Remover espaços extras no início/fim e múltiplos espaços internos\n",
    "    texto = re.sub(r'\\s+', ' ', texto).strip()\n",
    "    # Corrigir erros comuns remanescentes (ADICIONE MAIS AQUI CONFORME NECESSÁRIO)\n",
    "    texto = texto.replace('Prefi.xo', 'Prefixo')\n",
    "    texto = texto.replace('Substarítivo', 'Substantivo')\n",
    "    texto = texto.replace('S)ibsta\\'ntivo', 'Substantivo')\n",
    "    texto = texto.replace('Adj^fcivo', 'Adjetivo')\n",
    "    texto = texto.replace('Partlculas', 'Partículas')\n",
    "    texto = texto.replace('Patticulas', 'Partículas')\n",
    "    texto = texto.replace('Adverbio', 'Advérbio')\n",
    "    texto = texto.replace('Adverbiorse', 'Advérbio: se') # Exemplo\n",
    "    texto = texto.replace('náo', 'não')\n",
    "    texto = texto.replace('Nao', 'Não')\n",
    "    texto = texto.replace('fungáo', 'função')\n",
    "    texto = texto.replace('aqáo', 'ação')\n",
    "    texto = texto.replace('si^ nificado', 'significado') # Corrigir ^\n",
    "    texto = texto.replace('procedéncia', 'procedência')\n",
    "    texto = texto.replace('género', 'gênero')\n",
    "    texto = texto.replace('duplicagao', 'duplicação')\n",
    "    texto = texto.replace('duplicaqao', 'duplicação')\n",
    "    texto = texto.replace('multidao', 'multidão')\n",
    "    texto = texto.replace('ÍABA', 'ABA') # Provável\n",
    "    texto = texto.replace('f ', '† ') # Talvez usar um símbolo diferente para lusismo/apócope\n",
    "    texto = texto.replace('+ ', '‡ ') # Talvez usar um símbolo diferente\n",
    "    # Corrigir numeração inicial das definições\n",
    "    texto = re.sub(r'^\\s*\\^', '1.', texto, flags=re.MULTILINE) # ^ -> 1.\n",
    "    texto = re.sub(r'^\\s*z', '2.', texto, flags=re.MULTILINE) # z -> 2.\n",
    "    # Substituir 9 por ç (com cuidado)\n",
    "    texto = re.sub(r'\\b9\\b', 'ç', texto) # 9 isolado\n",
    "    texto = texto.replace('(t, 9 )', '(t, ç)')\n",
    "    texto = texto.replace('(Q—,<^—)', '(Q-, ç-)') # Suposição\n",
    "    texto = texto.replace('(£-)', '(S-)') # Suposição\n",
    "    return texto\n",
    "\n",
    "dados_estruturados = []\n",
    "\n",
    "if 'entradas_brutas' in locals() and entradas_brutas:\n",
    "    for i, entrada_bruta in enumerate(entradas_brutas):\n",
    "        entrada_limpa = limpar_texto(entrada_bruta)\n",
    "        linhas_entrada = entrada_limpa.splitlines()\n",
    "\n",
    "        if not linhas_entrada:\n",
    "            continue\n",
    "\n",
    "        verbete_completo = linhas_entrada[0].strip()\n",
    "        corpo_texto = \"\\n\".join(linhas_entrada[1:]).strip()\n",
    "\n",
    "        verbete_tupi = verbete_completo\n",
    "        marcadores = None\n",
    "        classe_gramatical = None\n",
    "        definicoes = []\n",
    "        exemplos = []\n",
    "        referencias = []\n",
    "        notas = []\n",
    "\n",
    "        # Tentar extrair marcadores do verbete\n",
    "        match_marcador = re.search(r'(\\s*[({\\[].*?[)}\\]])', verbete_completo)\n",
    "        if match_marcador:\n",
    "            marcadores = match_marcador.group(1).strip()\n",
    "            verbete_tupi = verbete_completo.replace(match_marcador.group(0), '').strip() # Remove marcador do verbete\n",
    "            verbete_tupi = re.sub(r'[.—]$', '', verbete_tupi).strip() # Remove . ou — do final se houver\n",
    "\n",
    "        # Tentar extrair classe gramatical (do fim do verbete ou início do corpo)\n",
    "        padrao_classe = r'([A-Z][a-zA-Zçáéíóúãõü ]+(?: [a-z]+)*)[:.]' # Ex: Substantivo:, Verbo transitivo irregular.\n",
    "        match_classe_verbete = re.search(padrao_classe + r'\\s*$', verbete_tupi) # No fim do verbete limpo\n",
    "        match_classe_corpo = re.match(padrao_classe, corpo_texto) # No início do corpo\n",
    "\n",
    "        if match_classe_verbete:\n",
    "            classe_gramatical = match_classe_verbete.group(1).strip()\n",
    "            verbete_tupi = verbete_tupi.replace(match_classe_verbete.group(0), '').strip()\n",
    "        elif match_classe_corpo:\n",
    "            classe_gramatical = match_classe_corpo.group(1).strip()\n",
    "            corpo_texto = corpo_texto[match_classe_corpo.end():].strip() # Remove classe do corpo\n",
    "\n",
    "        # Processar o corpo para definições, exemplos, referências\n",
    "        # Dividir por linhas novamente pode ajudar a encontrar padrões no início\n",
    "        linhas_corpo = corpo_texto.splitlines()\n",
    "        definicao_atual = \"\"\n",
    "        for linha in linhas_corpo:\n",
    "            linha = linha.strip()\n",
    "            if not linha: continue\n",
    "\n",
    "            # Verificar exemplos\n",
    "            match_ex = re.match(r'EX:\\s*(.*)', linha, re.IGNORECASE)\n",
    "            if match_ex:\n",
    "                if definicao_atual: definicoes.append(definicao_atual.strip())\n",
    "                definicao_atual = \"\"\n",
    "                exemplos.append(match_ex.group(1).strip())\n",
    "                continue\n",
    "\n",
    "            # Verificar referências\n",
    "            match_ref = re.match(r'=\\s*(.*)', linha)\n",
    "            if match_ref:\n",
    "                if definicao_atual: definicoes.append(definicao_atual.strip())\n",
    "                definicao_atual = \"\"\n",
    "                referencias.append(match_ref.group(1).strip())\n",
    "                continue\n",
    "\n",
    "            # Verificar início de nova definição numerada ou com símbolo\n",
    "            match_num = re.match(r'^([1-9]+\\.|[†‡])\\s*(.*)', linha)\n",
    "            if match_num:\n",
    "                if definicao_atual: definicoes.append(definicao_atual.strip())\n",
    "                definicao_atual = linha # Começa nova definição\n",
    "            else:\n",
    "                # Continuação da definição anterior ou a primeira definição\n",
    "                definicao_atual += \" \" + linha\n",
    "\n",
    "        # Adicionar a última definição acumulada\n",
    "        if definicao_atual:\n",
    "            definicoes.append(definicao_atual.strip())\n",
    "\n",
    "        # Limpar definições (remover números/símbolos iniciais se ainda presentes)\n",
    "        definicoes_limpas = []\n",
    "        for d in definicoes:\n",
    "             d_limpa = re.sub(r'^([1-9]+\\.|[†‡])\\s*', '', d).strip()\n",
    "             if d_limpa: definicoes_limpas.append(d_limpa)\n",
    "\n",
    "\n",
    "        # (Opcional/Avançado: Tentar separar Tupi/Português nos exemplos)\n",
    "\n",
    "        # Armazenar dados estruturados\n",
    "        if verbete_tupi or definicoes_limpas or exemplos or referencias: # Só adiciona se tiver algo útil\n",
    "            dados_estruturados.append({\n",
    "                'id': i, # Adiciona um ID simples\n",
    "                'verbete_original': verbete_completo, # Guardar o original pode ser útil\n",
    "                'verbete_tupi': verbete_tupi,\n",
    "                'marcadores': marcadores,\n",
    "                'classe_gramatical': classe_gramatical,\n",
    "                'definicoes': definicoes_limpas,\n",
    "                'exemplos': exemplos,\n",
    "                'referencias': referencias,\n",
    "                'texto_completo_entrada': entrada_limpa # Guarda o texto completo da entrada para referência\n",
    "            })\n",
    "\n",
    "    print(f\"\\nProcessadas {len(dados_estruturados)} entradas estruturadas.\")\n",
    "\n",
    "    # ----- Salvar os dados estruturados em JSON -----\n",
    "    try:\n",
    "        # Garante que o diretório de saída exista\n",
    "        output_dir_json = os.path.dirname(caminho_arquivo_json)\n",
    "        if not os.path.exists(output_dir_json):\n",
    "            os.makedirs(output_dir_json)\n",
    "\n",
    "        with open(caminho_arquivo_json, 'w', encoding='utf-8') as f_json:\n",
    "            # indent=2 torna o arquivo JSON legível por humanos\n",
    "            json.dump(dados_estruturados, f_json, ensure_ascii=False, indent=2)\n",
    "        print(f\"\\nDados estruturados salvos em '{caminho_arquivo_json}'\")\n",
    "\n",
    "        # Mostrar exemplo da primeira entrada estruturada\n",
    "        if dados_estruturados:\n",
    "            print(\"\\n--- Exemplo da Primeira Entrada Estruturada (JSON) ---\")\n",
    "            print(json.dumps(dados_estruturados[0], ensure_ascii=False, indent=2))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao salvar o arquivo JSON: {e}\")\n",
    "\n",
    "elif 'entradas_brutas' in locals() and not entradas_brutas:\n",
    "     print(\"A divisão em entradas brutas falhou ou não gerou resultados.\")\n",
    "else:\n",
    "     print(\"Execute os passos anteriores primeiro.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4829140f",
   "metadata": {},
   "source": [
    "### Validação e Refinamento do JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de6142d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON carregado com 7922 entradas.\n",
      "\n",
      "--- Amostra Aleatória de Entradas (DataFrame Pandas) ---\n",
      "        id  \\\n",
      "4530  4530   \n",
      "7671  7671   \n",
      "6950  6950   \n",
      "1933  1933   \n",
      "5139  5139   \n",
      "1991  1991   \n",
      "7651  7651   \n",
      "5879  5879   \n",
      "5968  5968   \n",
      "2688  2688   \n",
      "1787  1787   \n",
      "4811  4811   \n",
      "3521  3521   \n",
      "7125  7125   \n",
      "4744  4744   \n",
      "\n",
      "                                                                                     verbete_original_linha  \\\n",
      "4530                             MO-IO-BÃI-x0ARqA. Verbo transitivo: por um diante do outro. - mo-Io-bãi-a.   \n",
      "7671                                                   YBYT-I-MBORA. Substantivo: pó, poeira. = ybytu-byra.   \n",
      "6950                                                                  TAPERÃ. Ornitologia: andorinha grande   \n",
      "1933                                                                EK0-PORANGA (T-). Substantivo: virtude,   \n",
      "5139                       NDOaRA. Participio sufixo: o que costuma ser ou estar. S na- salacão de \"guara\".   \n",
      "1991                                                    M mbi-tyma,,, mas u mi-tyma,,: enterrado, plantado.   \n",
      "7651  YBYrA-PÓ_KANGA. Substantivo: cerca de defesa (externa) com troncos mais espagados do que os do ybvrã.   \n",
      "5879                                                       PEQE (XE). Verbo transitivo: principar, encetar.   \n",
      "5968                                              PÉ-YPY-KÓIA. Substantivo: encru2ilhada. = pé-Ie-atjapaba.   \n",
      "2688                        I-RAMEI. Particula comparativa: como ele, igual a ele (em tamanho e qualidade).   \n",
      "1787                                                EQÃ-PYÃ (T—). Substantivo^: presteza, Adjetivo: presto.   \n",
      "4811                                     MO-p0-g0aCJU. Verbo transitivo (tornar grosso muito): engros- sar.   \n",
      "3521                                                          KÓ-PIÃ-gO-ARA. Substantivo: o que vai ã roga.   \n",
      "7125                                                                                          TIN-GOAgU ti:   \n",
      "4744                                                MO-NGY-PAB-A. Verbo transitivo: gastar tudo, usar tudo.   \n",
      "\n",
      "                                                                                    verbete_tupi  \\\n",
      "4530                   MO-IO-BÃI-x0ARqA. Verbo transitivo: por um diante do outro. - mo-Io-bãi-a   \n",
      "7671                                         YBYT-I-MBORA. Substantivo: pó, poeira. = ybytu-byra   \n",
      "6950                                                       TAPERÃ. Ornitologia: andorinha grande   \n",
      "1933                                                          EK0-PORANGA. Substantivo: virtude,   \n",
      "5139             NDOaRA. Participio sufixo: o que costuma ser ou estar. S na- salacão de \"guara\"   \n",
      "1991                                          M mbi-tyma,,, mas u mi-tyma,,: enterrado, plantado   \n",
      "7651  YBYrA-PÓ_KANGA. Substantivo: cerca de defesa com troncos mais espagados do que os do ybvrã   \n",
      "5879                                                  PEQE. Verbo transitivo: principar, encetar   \n",
      "5968                                    PÉ-YPY-KÓIA. Substantivo: encru2ilhada. = pé-Ie-atjapaba   \n",
      "2688                                       I-RAMEI. Particula comparativa: como ele, igual a ele   \n",
      "1787                                           EQÃ-PYÃ. Substantivo^: presteza, Adjetivo: presto   \n",
      "4811                                                 MO-p0-g0aCJU. Verbo transitivo: engros- sar   \n",
      "3521                                                KÓ-PIÃ-gO-ARA. Substantivo: o que vai ã roga   \n",
      "7125                                                                                TIN-GOAgU ti   \n",
      "4744                                      MO-NGY-PAB-A. Verbo transitivo: gastar tudo, usar tudo   \n",
      "\n",
      "                    marcadores classe_gramatical definicoes exemplos  \\\n",
      "4530                      None              None         []       []   \n",
      "7671                      None              None         []       []   \n",
      "6950                      None              None         []       []   \n",
      "1933                      (T-)              None         []       []   \n",
      "5139                      None              None         []       []   \n",
      "1991                      None              None         []       []   \n",
      "7651                 (externa)              None         []       []   \n",
      "5879                      (XE)              None         []       []   \n",
      "5968                      None              None         []       []   \n",
      "2688  (em tamanho e qualidade)              None         []       []   \n",
      "1787                      (T—)              None         []       []   \n",
      "4811     (tornar grosso muito)              None         []       []   \n",
      "3521                      None              None         []       []   \n",
      "7125                      None              None         []       []   \n",
      "4744                      None              None         []       []   \n",
      "\n",
      "     referencias  \n",
      "4530          []  \n",
      "7671          []  \n",
      "6950          []  \n",
      "1933          []  \n",
      "5139          []  \n",
      "1991          []  \n",
      "7651          []  \n",
      "5879          []  \n",
      "5968          []  \n",
      "2688          []  \n",
      "1787          []  \n",
      "4811          []  \n",
      "3521          []  \n",
      "7125          []  \n",
      "4744          []  \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "caminho_arquivo_json = 'C:/Users/peterson.pereira/Documents/GitHub/Tupi2LLM/output/dicionario_estruturado_final.json'\n",
    "try:\n",
    "    with open(caminho_arquivo_json, 'r', encoding='utf-8') as f:\n",
    "        dados_estruturados = json.load(f)\n",
    "    print(f\"JSON carregado com {len(dados_estruturados)} entradas.\")\n",
    "\n",
    "    # Opcional: Converter para DataFrame Pandas para fácil visualização/filtragem\n",
    "    df = pd.DataFrame(dados_estruturados)\n",
    "    pd.set_option('display.max_rows', 100) # Mostrar mais linhas\n",
    "    pd.set_option('display.max_colwidth', 200) # Mostrar mais texto nas colunas\n",
    "\n",
    "    # Mostrar algumas entradas aleatórias\n",
    "    print(\"\\n--- Amostra Aleatória de Entradas (DataFrame Pandas) ---\")\n",
    "    print(df.sample(n=15)) # Veja 15 entradas aleatórias\n",
    "\n",
    "    # Mostrar as primeiras e últimas entradas também pode ser útil\n",
    "    # print(\"\\n--- Primeiras 5 Entradas ---\")\n",
    "    # print(df.head(5))\n",
    "    # print(\"\\n--- Últimas 5 Entradas ---\")\n",
    "    # print(df.tail(5))\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Erro: Arquivo JSON não encontrado em '{caminho_arquivo_json}'\")\n",
    "    dados_estruturados = None\n",
    "    df = None\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"Erro ao decodificar o JSON: {e}\")\n",
    "    dados_estruturados = None\n",
    "    df = None\n",
    "except Exception as e:\n",
    "     print(f\"Erro inesperado ao carregar/processar JSON: {e}\")\n",
    "     dados_estruturados = None\n",
    "     df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cfc337",
   "metadata": {},
   "source": [
    "### Geração do Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf4e6a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON carregado com 7922 entradas para gerar dataset TXT.\n",
      "Dataset de texto Tupi gerado em 'C:/Users/peterson.pereira/Documents/GitHub/Tupi2LLM/output/dataset_tupi.txt' com 4961 linhas.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "caminho_arquivo_json = 'C:/Users/peterson.pereira/Documents/GitHub/Tupi2LLM/output/dicionario_estruturado_final.json'\n",
    "caminho_dataset_txt = 'C:/Users/peterson.pereira/Documents/GitHub/Tupi2LLM/output/dataset_tupi.txt'\n",
    "dados_estruturados = []\n",
    "\n",
    "try:\n",
    "    with open(caminho_arquivo_json, 'r', encoding='utf-8') as f:\n",
    "        dados_estruturados = json.load(f)\n",
    "    print(f\"JSON carregado com {len(dados_estruturados)} entradas para gerar dataset TXT.\")\n",
    "\n",
    "    with open(caminho_dataset_txt, 'w', encoding='utf-8') as f_out:\n",
    "        count_linhas = 0\n",
    "        for entrada in dados_estruturados:\n",
    "            verbete = entrada.get('verbete_tupi')\n",
    "            exemplos_tupi = [] # Você precisaria implementar a extração dos exemplos em Tupi\n",
    "\n",
    "            # Escrever verbete (se válido)\n",
    "            if verbete and isinstance(verbete, str) and verbete.strip():\n",
    "                 # Pode adicionar filtros aqui (ex: ignorar verbetes muito curtos ou com caracteres estranhos)\n",
    "                 if len(verbete.split()) < 10: # Heurística simples para evitar definições coladas\n",
    "                    f_out.write(verbete.strip() + '\\n')\n",
    "                    count_linhas += 1\n",
    "\n",
    "            # Escrever exemplos em Tupi (se houver)\n",
    "            # for ex_tupi in exemplos_tupi:\n",
    "            #      if ex_tupi and isinstance(ex_tupi, str) and ex_tupi.strip():\n",
    "            #           f_out.write(ex_tupi.strip() + '\\n')\n",
    "            #           count_linhas += 1\n",
    "\n",
    "    print(f\"Dataset de texto Tupi gerado em '{caminho_dataset_txt}' com {count_linhas} linhas.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao gerar dataset TXT: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
